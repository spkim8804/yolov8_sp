{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2492b89-7e41-4d6e-a38a-afa0cec8c361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208 of 209 frames processed: 0.00 mins left\n",
      "Pose estimation complete!"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import sp_modules as sp\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('./weights/20240106_best_weight_yolov8_640.pt')\n",
    "\n",
    "# Open the video file\n",
    "# file_path = 'E:/Raw data/datasets/AVATAR/'\n",
    "# file_name = 'Hab_01_1200f'\n",
    "file_path = 'D:/spkim/data/20241114_40Hz_test/20241114_40Hz_test/'\n",
    "file_name = 'C_20Hz_Bandi_20fps'\n",
    "input_video_name = file_path + file_name + '.mp4'\n",
    "output_video_name = file_path + file_name + '_output.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_name)\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 files\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Use the same FPS as the source video\n",
    "frame_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Use the total frame to estimate the time\n",
    "frame_width = int(cap.get(3))  # Width of the frames in the video\n",
    "frame_height = int(cap.get(4))  # Height of the frames in the video\n",
    "frame_width_resize = 800\n",
    "frame_height_resize = 600\n",
    "\n",
    "out = cv2.VideoWriter(output_video_name, fourcc, fps, (frame_width, frame_height))\n",
    "# out = cv2.VideoWriter(output_video_name, fourcc, fps, (frame_width_resize, frame_height_resize))\n",
    "current_frame=0\n",
    "\n",
    "# Color code setting for center-point\n",
    "color_code = {\n",
    "    \"red\": (0, 0, 255),\n",
    "    \"orange\": (0, 165, 255),\n",
    "    \"yellow\": (0, 255, 255),\n",
    "    \"green\": (0, 128, 0),\n",
    "    \"blue\": (255, 0, 0),\n",
    "    \"skyblue\": (235, 206, 135),\n",
    "    \"purple\": (128, 0, 128),\n",
    "    \"black\": (0, 0, 0),\n",
    "    \"pink\": (255, 192, 203)\n",
    "}\n",
    "color_order = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"skyblue\", \"purple\", \"black\", \"pink\"]\n",
    "output = []\n",
    "\n",
    "#To match darknet (YOLOv4), #4, 5: forelimb / #6, 7: hindlimb\n",
    "rearrange = [4, 6, 0, 1, 2, 8, 3]\n",
    "# Five areas from captured frame (x1, y1, x2, y2)\n",
    "area = [\n",
    "    [ 1, 1, 1200, 1000],\n",
    "    [ 1201, 1, 2400, 1000],\n",
    "    [ 2401, 1, 3600, 1200],\n",
    "    [ 1, 1001, 1200, 2000],\n",
    "    [ 1201, 1001, 2400, 2000]\n",
    "]\n",
    "raw_coordinates = [0] * 135\n",
    "while cap.isOpened():\n",
    "    start_time = time.time() # Time measurement\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, verbose=False)\n",
    "                \n",
    "        # Visualize the results on the frame\n",
    "        # annotated_frame = results[0].plot(line_width = 1, labels = False)\n",
    "        annotated_frame = frame\n",
    "        \n",
    "        # Draw bounding box and center\n",
    "        check_cls = [[0 for _ in range(10)] for _ in range(10)]\n",
    "        raw_coordinates = [0] * 135\n",
    "        \n",
    "        for det in results[0].boxes:\n",
    "            # det: [x1, y1, x2, y2, conf, cls]\n",
    "            xy = det.xyxy.tolist()[0]\n",
    "            conf, cls = det.conf.item(), int(det.cls.item())\n",
    "            # cls definition\n",
    "            # 0: fore / 1: hind / 2: nose / 3: head / 4: ass / 5: tail / 6: torso\n",
    "\n",
    "            # Calculate the center of the bounding box\n",
    "            center_x, center_y = (int(xy[0]) + int(xy[2])) // 2, (int(xy[1]) + int(xy[3])) // 2\n",
    "            \n",
    "            # Get only one xy pair (highest conf value) in each camera view\n",
    "            for i in range(5):\n",
    "                # Find the ROI (camera 0 to 4)\n",
    "                if(center_x > area[i][0] and center_x < area[i][2] and center_y > area[i][1] and center_y < area[i][3]):\n",
    "                    # forehand\n",
    "                    if(cls == 0 or cls == 1): # 0: fore / 1: hind\n",
    "                        if(check_cls[i][rearrange[cls]] == 0):\n",
    "                            raw_coordinates[i*27 + rearrange[cls]*3] = center_x\n",
    "                            raw_coordinates[i*27 + 1 + rearrange[cls]*3] = center_y\n",
    "                            raw_coordinates[i*27 + 2 + rearrange[cls]*3] = conf\n",
    "                            check_cls[i][rearrange[cls]] += 1\n",
    "                        elif(check_cls[i][rearrange[cls]] == 1):\n",
    "                            raw_coordinates[i*27 + rearrange[cls]*3 + 3] = center_x\n",
    "                            raw_coordinates[i*27 + 1 + rearrange[cls]*3 + 3] = center_y\n",
    "                            raw_coordinates[i*27 + 2 + rearrange[cls]*3 + 3] = conf\n",
    "                            check_cls[i][rearrange[cls]] += 1\n",
    "                    elif(check_cls[i][rearrange[cls]] == 0): # Rest of body points\n",
    "                        raw_coordinates[i*27 + rearrange[cls]*3] = center_x\n",
    "                        raw_coordinates[i*27 + 1 + rearrange[cls]*3] = center_y\n",
    "                        raw_coordinates[i*27 + 2 + rearrange[cls]*3] = conf\n",
    "                        check_cls[i][rearrange[cls]] += 1\n",
    "\n",
    "            # Optionally draw the bounding box\n",
    "            cv2.rectangle(annotated_frame, (int(xy[0]), int(xy[1])), (int(xy[2]), int(xy[3])), color=(255, 0, 0), thickness=2)\n",
    "        \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    # Measure an angle of each limb from body-ass line (4, 5: forelimb / 6, 7: hindlimb / 2: body / 3: ass)\n",
    "\n",
    "    # forelimb (54 is for bottom camera)\n",
    "    f1_angle = sp.get_AB_AC_angle(raw_coordinates[54 + 3*3], raw_coordinates[54 + 3*3 +1],\n",
    "                               raw_coordinates[54 + 2*3], raw_coordinates[54 + 2*3 +1],\n",
    "                               raw_coordinates[54 + 4*3], raw_coordinates[54 + 4*3 +1])\n",
    "\n",
    "    f2_angle = sp.get_AB_AC_angle(raw_coordinates[54 + 3*3], raw_coordinates[54 + 3*3 +1],\n",
    "                               raw_coordinates[54 + 2*3], raw_coordinates[54 + 2*3 +1],\n",
    "                               raw_coordinates[54 + 5*3], raw_coordinates[54 + 5*3 +1])\n",
    "    \n",
    "    if(f1_angle < 180 and f2_angle > 180):        \n",
    "        temp = raw_coordinates[54 + 4*3 : 54 + 4*3 +2]\n",
    "        raw_coordinates[54 + 4*3: 54 + 4*3 +2] = raw_coordinates[54 +5*3 : 54 +5*3 +2]\n",
    "        raw_coordinates[54 + 5*3 : 54+ 5*3 +2] = temp\n",
    "    \n",
    "    # hindlimb\n",
    "    h1_angle = sp.get_AB_AC_angle(raw_coordinates[54 + 3*3], raw_coordinates[54 + 3*3+1],\n",
    "                               raw_coordinates[54 + 2*3], raw_coordinates[54 + 2*3+1],\n",
    "                               raw_coordinates[54 + 6*3], raw_coordinates[54 + 6*3+1])\n",
    "\n",
    "    h2_angle = sp.get_AB_AC_angle(raw_coordinates[54 + 3*3], raw_coordinates[54 + 3*3+1],\n",
    "                                   raw_coordinates[54 + 2*3], raw_coordinates[54 + 2*3+1],\n",
    "                                   raw_coordinates[54 + 7*3], raw_coordinates[54 + 7*3+1])\n",
    "    \n",
    "    if(h1_angle < 180 and h2_angle > 180):\n",
    "        temp = raw_coordinates[54 + 6*3 : 54 + 6*3+2]\n",
    "        raw_coordinates[54 + 6*3 : 54 + 6*3+2] = raw_coordinates[54 +7*3 : 54 + 7*3+2]\n",
    "        raw_coordinates[54 + 7*3 : 54 + 7*3+2] = temp\n",
    "\n",
    "    # Draw a center point of each feature\n",
    "    for i in range(5):\n",
    "        for j in range(9):\n",
    "            if(raw_coordinates[i*27+j*3] != 0):\n",
    "                cv2.circle(annotated_frame, (raw_coordinates[i*27+j*3], raw_coordinates[i*27+j*3+1]),\n",
    "                        radius=10, color=color_code[color_order[j]], thickness=-1)\n",
    "\n",
    "    # Display the frame with bounding box and center\n",
    "    # resized_frame = cv2.resize(annotated_frame, (800, 600))\n",
    "    out.write(annotated_frame)\n",
    "    # cv2.imshow(\"YOLOv8 Inference\", resized_frame)\n",
    "\n",
    "    # Write the annotated frame to the output video file\n",
    "    out.write(annotated_frame)\n",
    "    \n",
    "    # Result accumulation\n",
    "    output.append(raw_coordinates)\n",
    "    \n",
    "    # Print total estimated time\n",
    "    sp.print_remaining_time(start_time, time.time()-start_time, current_frame, frame_total)\n",
    "    \n",
    "    current_frame += 1\n",
    "\n",
    "# Print job completion\n",
    "if(current_frame == frame_total):\n",
    "    print('\\r', flush=True) # Make sure the blank\n",
    "    print(\"Pose estimation complete!\", end='\\r', flush=True)\n",
    "\n",
    "# Save to txt file\n",
    "with open(file_path+file_name+'.txt', 'w') as file:\n",
    "    for row in output:\n",
    "        file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53de2c-4bc1-4832-9b19-c9eec8efe61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
