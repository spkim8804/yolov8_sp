{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9bbcce-7152-4106-b3b4-5bbac5b87dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1196 of 1197 frames processed: 0.00 mins left\n",
      "Pose estimation complete!\r"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "import sp_modules as sp\n",
    "import numpy as np\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model = YOLO('./weights/20240106_best_weight_yolov8_640.pt')\n",
    "\n",
    "# Open the video file\n",
    "file_path = 'E:/Raw data/datasets/AVATAR/BYUN_data/'\n",
    "file_name = '2023-11-29_11-52-14-041_1200f'\n",
    "# file_path = 'E:/Raw data/datasets/AVATAR/age_mbest1_cerebellum/original_datasets/collection/'\n",
    "# file_name = 'Hab_02_1200f'\n",
    "input_video_name = file_path + file_name + '.mp4'\n",
    "output_video_name = file_path + file_name + '_face.mp4'\n",
    "\n",
    "cap = cv2.VideoCapture(input_video_name)\n",
    "\n",
    "# Define the codec and create VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 files\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)  # Use the same FPS as the source video\n",
    "frame_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) # Use the total frame to estimate the time\n",
    "frame_width = int(cap.get(3))  # Width of the frames in the video\n",
    "frame_height = int(cap.get(4))  # Height of the frames in the video\n",
    "frame_width_resize = 800\n",
    "frame_height_resize = 600\n",
    "\n",
    "# out = cv2.VideoWriter(output_video_name, fourcc, fps, (frame_width, frame_height))\n",
    "out = cv2.VideoWriter(output_video_name, fourcc, fps, (1500, 300))\n",
    "current_frame=0\n",
    "\n",
    "# Color code setting for center-point\n",
    "color_code = {\n",
    "    \"red\": (0, 0, 255),\n",
    "    \"orange\": (0, 165, 255),\n",
    "    \"yellow\": (0, 255, 255),\n",
    "    \"green\": (0, 128, 0),\n",
    "    \"blue\": (255, 0, 0),\n",
    "    \"skyblue\": (235, 206, 135),\n",
    "    \"purple\": (128, 0, 128),\n",
    "    \"black\": (0, 0, 0),\n",
    "    \"pink\": (255, 192, 203)\n",
    "}\n",
    "color_order = [\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"skyblue\", \"purple\", \"black\", \"pink\"]\n",
    "output = []\n",
    "\n",
    "#To match darknet (YOLOv4), #4, 5: forelimb / #6, 7: hindlimb\n",
    "rearrange = [4, 6, 0, 1, 3, 8, 2]\n",
    "# Five areas from captured frame (x1, y1, x2, y2)\n",
    "area = [\n",
    "    [ 1, 1, 1200, 1000],\n",
    "    [ 1201, 1, 2400, 1000],\n",
    "    [ 2401, 1, 3600, 1200],\n",
    "    [ 1, 1001, 1200, 2016],\n",
    "    [ 1201, 1001, 2400, 2016]\n",
    "]\n",
    "raw_coordinates = [0] * 135\n",
    "while cap.isOpened():\n",
    "    start_time = time.time() # Time measurement\n",
    "    head_image = np.zeros((300, 1500, 3), dtype=\"uint8\")\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame, verbose=False)\n",
    "                \n",
    "        # Visualize the results on the frame\n",
    "        # annotated_frame = results[0].plot(line_width = 1, labels = False)\n",
    "        annotated_frame = frame\n",
    "        cnt = 0\n",
    "        for det in results[0].boxes:\n",
    "            # det: [x1, y1, x2, y2, conf, cls]\n",
    "            xy = det.xyxy.tolist()[0]\n",
    "            conf, cls = det.conf.item(), int(det.cls.item())\n",
    "            # cls definition\n",
    "            # 0: fore / 1: hind / 2: nose / 3: head / 4: ass / 5: tail / 6: torso\n",
    "\n",
    "            if(cls == 3 and cnt < 5):\n",
    "                cropped_frame = annotated_frame[int(xy[1]):int(xy[3]), int(xy[0]):int(xy[2])]\n",
    "                # cv2.rectangle(annotated_frame, (int(xy[0]), int(xy[1])), (int(xy[2]), int(xy[3])), color=(255, 0, 0), thickness=2)\n",
    "                head_image[0:cropped_frame.shape[0], cnt*300:cnt*300+cropped_frame.shape[1]] = cropped_frame\n",
    "                cnt+=1\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "        \n",
    "        cv2.imshow(\"YOLOv8 Inference\", head_image)\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "    # Display the frame with bounding box and center\n",
    "    # resized_frame = cv2.resize(annotated_frame, (800, 600))\n",
    "    out.write(head_image)\n",
    "    # cv2.imshow(\"YOLOv8 Inference\", resized_frame)\n",
    "\n",
    "    # Write the annotated frame to the output video file\n",
    "    # out.write(annotated_frame)\n",
    "    \n",
    "    # Result accumulation\n",
    "    # output.append(raw_coordinates)\n",
    "    \n",
    "    # Print total estimated time\n",
    "    sp.print_remaining_time(start_time, time.time()-start_time, current_frame, frame_total)\n",
    "    \n",
    "    current_frame += 1\n",
    "\n",
    "# Print job completion\n",
    "if(current_frame == frame_total):\n",
    "    print('\\r', flush=True) # Make sure the blank\n",
    "    print(\"Pose estimation complete!\", end='\\r', flush=True)\n",
    "\n",
    "# Save to txt file\n",
    "# with open(file_path+file_name+'.txt', 'w') as file:\n",
    "#     for row in output:\n",
    "#         file.write('\\t'.join(map(str, row)) + '\\n')\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e073681-2ff6-4fa6-9ba7-90953d3a5ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
